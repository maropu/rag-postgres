{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de95cb8-9c67-4f89-b881-506a8c7c841b",
   "metadata": {},
   "source": [
    "# References\n",
    " - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
    "   - https://research.facebook.com/publications/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks/\n",
    " - LangChain documents\n",
    "   - https://python.langchain.com/docs/modules/chains\n",
    " - RAG(Retrieval Augmented Generation)を用いたPostgreSQLアシスタントAIの試作\n",
    "   - https://qiita.com/comware_hiratsuka/items/7fbc9df423dd64160f73 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2eeb1-ded8-467f-8e5c-dbca1d2b0db0",
   "metadata": {},
   "source": [
    "# Dependency info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721cd98-f91e-4026-9e4f-1073b876f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check required dependencies, see `requirements.txt`\n",
    "# !pip install -r repo/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4526dfe-3d34-4ac7-89b4-e4ae8e3b142b",
   "metadata": {},
   "source": [
    "# Plantform info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dfe5828-4fcc-4836-8cd4-9e47a98fbeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor\t: 0\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 126\n",
      "model name\t: 06/7e\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1\n",
      "cpu MHz\t\t: 2303.604\n",
      "cache size\t: 16384 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 2\n",
      "core id\t\t: 0\n",
      "cpu cores\t: 2\n",
      "apicid\t\t: 0\n",
      "initial apicid\t: 0\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 13\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology cpuid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch pti fsgsbase bmi1 avx2 smep bmi2 erms avx512f avx512dq rdseed adx smap avx512ifma clflushopt avx512cd sha_ni avx512bw avx512vl xsaveopt xgetbv1 arat avx512vbmi avx512_vpopcntdq rdpid\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit srbds mmio_stale_data retbleed gds\n",
      "bogomips\t: 4607.20\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 36 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n",
      "processor\t: 1\n",
      "vendor_id\t: GenuineIntel\n",
      "cpu family\t: 6\n",
      "model\t\t: 126\n",
      "model name\t: 06/7e\n",
      "stepping\t: 5\n",
      "microcode\t: 0x1\n",
      "cpu MHz\t\t: 2303.604\n",
      "cache size\t: 16384 KB\n",
      "physical id\t: 0\n",
      "siblings\t: 2\n",
      "core id\t\t: 1\n",
      "cpu cores\t: 2\n",
      "apicid\t\t: 1\n",
      "initial apicid\t: 1\n",
      "fpu\t\t: yes\n",
      "fpu_exception\t: yes\n",
      "cpuid level\t: 13\n",
      "wp\t\t: yes\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology cpuid pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch pti fsgsbase bmi1 avx2 smep bmi2 erms avx512f avx512dq rdseed adx smap avx512ifma clflushopt avx512cd sha_ni avx512bw avx512vl xsaveopt xgetbv1 arat avx512vbmi avx512_vpopcntdq rdpid\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit srbds mmio_stale_data retbleed gds\n",
      "bogomips\t: 4607.20\n",
      "clflush size\t: 64\n",
      "cache_alignment\t: 64\n",
      "address sizes\t: 36 bits physical, 48 bits virtual\n",
      "power management:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59698210-d7af-47d3-815c-c53dd46faa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:            11Gi       438Mi       9.7Gi       5.0Mi       1.6Gi        10Gi\n",
      "Swap:             0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06674003-8c3b-45b7-b142-cba41964a98a",
   "metadata": {},
   "source": [
    "# The next cell is used to parse the PostgreSQL documents and generate input documents for external knowledge in RAG\n",
    " - A parsing strategy is to parse a table of contents in \"index.html\" and follow hyperlinks contained there for keeping document structures\n",
    "   - See `generated/postgres-doc_ja_v15.json` for the output example of this cell\n",
    " - TODO\n",
    "   - Append metadata tags into parsed sentences; for instance, a sentence having SQL commands is attached with the tag that represents its meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72f94a5e-bd4c-440b-a4da-12ae7b5cd50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1054/1054 [00:21<00:00, 49.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1897 documents collected, written into postgres-doc_ja_v15.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import copy\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import tqdm\n",
    "from lxml import etree\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "# You can change this variable for your input data\n",
    "# NB: Needs to check if target HTML resources exist in `dataset`\n",
    "POSTGRES_DOC = 'repo/dataset/postgres-doc_ja_v15'\n",
    "\n",
    "\n",
    "def collect_html_files_from(ref: str, titles: list[str] = [], files: dict[str, list[str]] = {}) -> dict[str, list[str]]:\n",
    "    soup = bs4.BeautifulSoup(Path(f\"{POSTGRES_DOC}/{ref}\").read_text(), 'html.parser')\n",
    "    \n",
    "    toc = soup.find(\"div\", {\"class\": \"toc\"})\n",
    "    if toc is None:\n",
    "        if files.get(ref) is None or len(files[ref]) < len(titles):\n",
    "            files[ref] = titles\n",
    "\n",
    "        return files\n",
    "\n",
    "    if len(titles) == 0 or titles[-1] != title:\n",
    "        titles = copy.deepcopy(titles)\n",
    "        titles.append(soup.find('title').text)\n",
    "\n",
    "    for e in etree.fromstring(str(toc)).iter():\n",
    "        # Skip redundant links, e.g., xxx.html#yyyy\n",
    "        if e.tag == 'a' and \"#\" not in e.get('href'):\n",
    "            collect_html_files_from(e.get('href'), titles, files)\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def remove_html_tags(s: str) -> str:\n",
    "    s = re.sub(re.compile('<.*?>'), ' ', s)\n",
    "    s = re.sub(\"<!--.+?-->\", ' ', s, flags=re.DOTALL)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def has_subsect(elem: Any, i: int) -> bool:\n",
    "    # TODO: Why can't we do it like `elem.findall(\"div\", {\"class\": f\"sect{i+1}\"})` here?\n",
    "    for sect in elem.findall(\"div\"):\n",
    "        if sect.get('class') == f\"sect{i}\":\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def collect_docs(elem: Any, i: int, ref: str, titles: list[str], docs: list[Any]) -> None:\n",
    "    for sect in elem.findall(\"div\", {\"class\": f\"sect{i}\"}):\n",
    "        if sect.get('class') not in (f\"sect{i}\", 'chapter', 'refentry', 'appendix', 'bibliography'):\n",
    "            continue\n",
    "\n",
    "        current_titles = copy.deepcopy(titles)\n",
    "        for e in sect.iter():\n",
    "            if e.tag == f\"h{i+1}\":\n",
    "                current_titles.append(remove_html_tags(etree.tostring(e, encoding=str)))\n",
    "\n",
    "        if has_subsect(sect, i+1):\n",
    "            collect_docs(sect, i+1, ref, current_titles, docs)\n",
    "        else:\n",
    "            content = remove_html_tags(etree.tostring(sect, encoding=str))\n",
    "            titles_as_str = \"[{}]\".format(\", \".join(map(lambda x: f\"\\\"{x}\\\"\", current_titles)))\n",
    "            docs.append({\"content\": content, \"ref\": ref, \"titles\": titles_as_str})\n",
    "\n",
    "\n",
    "def collect_docs_in(ref: str, titles: list[str], docs: list[Any]) -> None:\n",
    "    soup = bs4.BeautifulSoup(Path(f\"{POSTGRES_DOC}/{ref}\").read_text(), 'html.parser')\n",
    "    doc = soup.find(\"body\", {\"id\": \"docContent\"})\n",
    "    root = etree.fromstring(str(doc))\n",
    "    collect_docs(root, 1, ref, titles, docs)\n",
    "\n",
    "\n",
    "# A variable to collect input documents for a vector store (e.g., faiss)\n",
    "docs = []\n",
    "\n",
    "for ref, titles in tqdm.tqdm(collect_html_files_from('index.html').items()):\n",
    "    collect_docs_in(ref, titles, docs)\n",
    "\n",
    "output_docs_filename = f\"{POSTGRES_DOC.split('/')[-1]}.json\"\n",
    "\n",
    "with open(output_docs_filename, 'w', encoding='utf-8') as out:\n",
    "\tjson.dump(docs, out, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"{len(docs)} documents collected, written into {output_docs_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc78197-7868-4f3e-bddd-9dc5e1e4e556",
   "metadata": {},
   "source": [
    "## The next cells are used to initialize a vector store with the collected documents above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c73f911-5351-4f04-a654-a7ea6126fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input documents first\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# You can change this variable for your input data of a vector store\n",
    "INPUT_DOCS = 'repo/generated/postgres-doc_ja_v15.json'\n",
    "\n",
    "\n",
    "def metadata_func(r: dict, md: dict) -> dict:\n",
    "    md[\"ref\"] = r.get(\"ref\")\n",
    "    md[\"titles\"] = r.get(\"titles\")\n",
    "    return md\n",
    "\n",
    "\n",
    "loader = JSONLoader(file_path=INPUT_DOCS, jq_schema='.[]', content_key='content', metadata_func=metadata_func)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fabdb6a-a265-4f97-a923-20db168edcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0f524fd38d4f4c8b2b358be43895e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9ede2b7c6e40818107fc1d174be473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d3814a43214b2ab5c5a9b018d10425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/179k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4b6c1115c64c5e9d44be1933c974cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d6eac5006c4afdaa805e0d7eb1c898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d294094b227d474aa2b214e0a56de84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcba2b6661e74322837f9cf690f2a6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c58ecadf72f47de8a4d8b5a29861154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a681da1e18427182bd195eb3940643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6980d8258b248f48f616ab87457b6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f1e47de0e74b0f9effdda9499a2ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c09f995bfe4023b94e57e026e52d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e4f0679a6f43a7b0d54bd3b1a44400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adad8ef871e4ea28a663f7a537f99b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b74dd44c3804a848428bbb72cd11a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52004d2a39874f35a0e2cd9da3cbcb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46df35326d24db6b8c451f79c957bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf6ff9ca56246debe4121b1a733a3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# This model has 24 layers, the embedding size is 768, and the maximum # of tokens is 512,\n",
    "# for more details, see https://huggingface.co/intfloat/multilingual-e5-base\n",
    "#\n",
    "# TODO: Replace the model with `intfloat/multilingual-e5-large` for more practical use:\n",
    "#   - https://huggingface.co/intfloat/multilingual-e5-large\n",
    "#\n",
    "# TODO: Try the embeddiing model that has the larger maximum # of tokens, for instance,\n",
    "# the number in `NousResearch/Yarn-Llama-2-13b-128k` is 128k, see the link below for more details:\n",
    "#   - https://huggingface.co/NousResearch/Yarn-Llama-2-13b-128k\n",
    "#\n",
    "# TODO: Try one of the most popular embedding models, `text-embedding-ada-002`\n",
    "# (embedding size is 1536 and the maximum # of tokens is 2046) in OpenAI,\n",
    "# because the PostgreSQL documents are public.\n",
    "#   - https://platform.openai.com/docs/guides/embeddings\n",
    "HUGGING_FACE_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
    "EMBEDDING = HuggingFaceEmbeddings(model_name=HUGGING_FACE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae47adaa-4cee-4fef-93ab-7620dfacac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1083, which is longer than the specified 1000\n",
      "Created a chunk of size 1287, which is longer than the specified 1000\n",
      "Created a chunk of size 1100, which is longer than the specified 1000\n",
      "Created a chunk of size 2603, which is longer than the specified 1000\n",
      "Created a chunk of size 5596, which is longer than the specified 1000\n",
      "Created a chunk of size 14145, which is longer than the specified 1000\n",
      "Created a chunk of size 1127, which is longer than the specified 1000\n",
      "Created a chunk of size 1269, which is longer than the specified 1000\n",
      "Created a chunk of size 3504, which is longer than the specified 1000\n",
      "Created a chunk of size 2096, which is longer than the specified 1000\n",
      "Created a chunk of size 1587, which is longer than the specified 1000\n",
      "Created a chunk of size 1114, which is longer than the specified 1000\n",
      "Created a chunk of size 1232, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token length: 1.5988751254864615\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Average token length: 2.381185373887978 (name='intfloat/multilingual-e5-base')\n",
    "def compute_avg_token_length_using_hf_tokenizer(docs: list, name=HUGGING_FACE_MODEL_NAME)-> float:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    num_tokens = 0\n",
    "    length = 0\n",
    "    for doc in docs:\n",
    "        tokens = tokenizer.encode(doc.page_content)\n",
    "        length += len(doc.page_content)\n",
    "        num_tokens += len(tokens)\n",
    "\n",
    "    return length / num_tokens\n",
    "\n",
    "\n",
    "# Average token length: 1.5988751254864615 (name='cl100k_base')\n",
    "def compute_avg_token_length_usinig_tiktoken(docs: list, name='cl100k_base') -> float:\n",
    "    import tiktoken\n",
    "    # cl100k_base: gpt-4, gpt-3.5-turbo, text-embedding-ada-002\n",
    "    tiktoken_encoding = tiktoken.get_encoding(name)\n",
    "    num_tokens = 0\n",
    "    length = 0\n",
    "    for doc in docs:\n",
    "        tokens = tiktoken_encoding.encode(doc.page_content)\n",
    "        length += len(doc.page_content)\n",
    "        num_tokens += len(tokens)\n",
    "\n",
    "    return length / num_tokens\n",
    "\n",
    "\n",
    "# Split page contents in given documents into smaller parts if the size of the contents goes over the limit (`chunk_size`),\n",
    "# see `generated/postgres-doc_ja_v15_split_doc_example.txt` for a split document example.\n",
    "#\n",
    "# A splitting strategy relies on the maximum number of tokens accepted in embedding and chat models.\n",
    "# In the current implementation, it is 512 in the embedding model and 1024? in the chat model, so\n",
    "# we set a chunk size at 1000 because we expect that the embedding model could accept\n",
    "# 512 * 2.38 (averaged chars per token) = ~1218 UTF-8 Japanese characters.\n",
    "#\n",
    "# TODO: Tried a two-stage (child & parent) splitter using `ParentDocumentRetriever` and contextual compression\n",
    "# using `ContextualCompressionRetriever`, but employed a native splitting strategy for now because of simplicity.\n",
    "# IIUC this strategy highly affects accuracy, so we need to tweak it for smarter splitting.\n",
    "def split_docs(docs: list, chunk_size: int, chunk_overlap: int) -> list:\n",
    "    import functools\n",
    "    splitters = []\n",
    "    splitters.append(CharacterTextSplitter(separator=\"\\n\", chunk_size=chunk_size, chunk_overlap=chunk_overlap))\n",
    "    splitters.append(RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap))\n",
    "    docs = functools.reduce(lambda docs, s: s.split_documents(docs), splitters, docs)\n",
    "    return docs\n",
    "\n",
    "\n",
    "avg_token_length = compute_avg_token_length_using_hf_tokenizer(docs)\n",
    "# avg_token_length = compute_avg_token_length_usinig_tiktoken(docs)\n",
    "print(f\"Average token length: {avg_token_length}\")\n",
    "\n",
    "# TODO: Tune the two parameters below, `chunk_size` and `chunk_overlap`\n",
    "docs = split_docs(docs, chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91eb2da5-8d33-4084-b33f-73668c27e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appends a header (ref, titles, ...) into the split docs\n",
    "# TODO: This addition might go over the limit of the context window size of the embedding model\n",
    "for doc in docs:\n",
    "    header = []\n",
    "    header.append(\"# ref: {}\".format(doc.metadata['ref']))\n",
    "    header.append(\"# titles: {}\".format(\"/\".join(json.loads(doc.metadata['titles']))))\n",
    "    doc.page_content = \"{}\\n\\n{}\".format(\"\\n\".join(header), doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27886f36-9335-404e-9d87-95e65e8df056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "index = FAISS.from_documents(docs, EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64b317f1-e1d4-4ecb-8634-481d46487994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as out:\n",
    "        pickle.dump(obj, out, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as in_:\n",
    "        return pickle.load(in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03219098-d305-41ca-a7c2-443422ecf877",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_filename = INPUT_DOCS.split(\"/\")[-1].replace(\".json\", \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19bfb11a-00cb-4e9a-be7c-0d659fc81e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Probably, we'd be better to use `.save_local()`/`.load_local()` instead?\n",
    "# pickle stores all the state as is, that is, some confidencial info, e.g., OPENAI_API_KEY\n",
    "\n",
    "# index.save_local(index_filenam)\n",
    "save_object(index, index_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2407676-3c0b-454c-91e9-fc51bfc60f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'source': '/home/jovyan/repo/generated/postgres-doc_ja_v15.json', 'seq_num': 1791, 'ref': 'pgtrgm.html', 'titles': '[\"PostgreSQL 15.4文書\", \"F.35. pg_trgm\", \"F.35.4. インデックスサポート\"]'}\n",
      "Metadata: {'source': '/home/jovyan/repo/generated/postgres-doc_ja_v15.json', 'seq_num': 1, 'ref': 'intro-whatis.html', 'titles': '[\"PostgreSQL 15.4文書\", \"1.  PostgreSQL とは?\"]'}\n",
      "Metadata: {'source': '/home/jovyan/repo/generated/postgres-doc_ja_v15.json', 'seq_num': 1379, 'ref': 'catalog-pg-index.html', 'titles': '[\"PostgreSQL 15.4文書\", \"53.26.  pg_index\"]'}\n",
      "Metadata: {'source': '/home/jovyan/repo/generated/postgres-doc_ja_v15.json', 'seq_num': 1379, 'ref': 'catalog-pg-index.html', 'titles': '[\"PostgreSQL 15.4文書\", \"53.26.  pg_index\"]'}\n",
      "Metadata: {'source': '/home/jovyan/repo/generated/postgres-doc_ja_v15.json', 'seq_num': 306, 'ref': 'indexes-examine.html', 'titles': '[\"PostgreSQL 15.4文書\", \"11.12. インデックス使用状況の検証\"]'}\n",
      "Metadata: {'source': '/home/jovyan/repo/generated/postgres-doc_ja_v15.json', 'seq_num': 348, 'ref': 'locking-indexes.html', 'titles': '[\"PostgreSQL 15.4文書\", \"13.7. ロックとインデックス\"]'}\n",
      "Metadata: {'source': '/home/jovyan/repo/generated/postgres-doc_ja_v15.json', 'seq_num': 291, 'ref': 'indexes-types.html', 'titles': '[\"PostgreSQL 15.4文書\", \"11.2. インデックスの種類\", \"11.2.1. B-Tree\"]'}\n",
      "Metadata: {'source': '/home/jovyan/repo/generated/postgres-doc_ja_v15.json', 'seq_num': 290, 'ref': 'indexes-intro.html', 'titles': '[\"PostgreSQL 15.4文書\", \"11.1. 序文\"]'}\n",
      "Metadata: {'source': '/home/jovyan/repo/generated/postgres-doc_ja_v15.json', 'seq_num': 1791, 'ref': 'pgtrgm.html', 'titles': '[\"PostgreSQL 15.4文書\", \"F.35. pg_trgm\", \"F.35.4. インデックスサポート\"]'}\n",
      "Metadata: {'source': '/home/jovyan/repo/generated/postgres-doc_ja_v15.json', 'seq_num': 583, 'ref': 'monitoring-stats.html', 'titles': '[\"PostgreSQL 15.4文書\", \"28.2. 累積統計システム\", \"28.2.20.  pg_statio_all_indexes\"]'}\n"
     ]
    }
   ],
   "source": [
    "# Check if the built vector store works well\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# index = FAISS.load_local(\"postgresql-doc-15-index\", EMBEDDING)\n",
    "index = load_object(index_filename)\n",
    "\n",
    "qes = \"PostgreSQLが実装しているインデックスの種類は？\"\n",
    "results = index.similarity_search(qes, k=10)\n",
    "for doc in results:\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3915e4-1102-44ac-b699-6d1601fe598a",
   "metadata": {},
   "source": [
    "# The next cells are used to initialize a chat model and do querying for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "df5f4d7c-f74c-4d7f-842e-fdcc65a7ef6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.ChatOpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace it with an open LLM model, e.g., `elyza/ELYZA-japanese-Llama-2-13b`\n",
    "#  - https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "MAX_NUM_TOKENS = 4096\n",
    "\n",
    "def count_tokens(s: str) -> int:\n",
    "    def _count_tokens_using_tiktoken(s: str, name='cl100k_base') -> int:\n",
    "        import tiktoken\n",
    "        # cl100k_base: gpt-4, gpt-3.5-turbo, text-embedding-ada-002\n",
    "        return len(tiktoken.get_encoding(name).encode(s))\n",
    "\n",
    "    return _count_tokens_using_tiktoken(s)\n",
    "\n",
    "\n",
    "# The maximum number of tokens in 'gpt-3.5-turbo' is 4096, so the number of context docs embedded in a prompt is ~8.\n",
    "# Newer models in OpenAI accept the larger number of tokens, e.g., one in 'gpt-4-1106-preview' is 128k, so\n",
    "# the number of embeded docs could increase accordingly.\n",
    "#\n",
    "# See a link below for the other models implemented in OpenAI:\n",
    "#  - https://platform.openai.com/docs/models\n",
    "OPENAI_MODEL_NAME = 'gpt-3.5-turbo'\n",
    "# OPENAI_MODEL_NAME = 'gpt-4'\n",
    "OPENAI_API_KEY = 'YOUR_OPENAI_KEY'\n",
    "\n",
    "llm = ChatOpenAI(model_name=OPENAI_MODEL_NAME, temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4403d4bc-a210-48d5-8a9e-ef6a1e76cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change this variable for your indexed documents in a vector store\n",
    "index = FAISS.load_local('repo/generated/postgresql-doc-15-index', EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c9c3d8da-ecb6-4b18-83d4-cf056c9f4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "qes = \"PostgreSQL 15で実装された新しい機能を箇条書きで可能な限り教えてください\"\n",
    "# qes = \"PostgreSQL 15におけるPUBLICスキーマの仕様変更について教えてください\"\n",
    "# qes = \"PostgreSQLでクエリの性能が悪いです。どのように調査して改善することができますか\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "687f005d-cca7-4331-830a-7378879de30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Context (n=6, num_tokens=3538) ==========\n",
      "\n",
      "0: ref=runtime-config-developer.html, titles=[\"PostgreSQL 15.4文書\", \"20.17. 開発者向けのオプション\"]\n",
      "1: ref=runtime-config-developer.html, titles=[\"PostgreSQL 15.4文書\", \"20.17. 開発者向けのオプション\"]\n",
      "2: ref=runtime-config-developer.html, titles=[\"PostgreSQL 15.4文書\", \"20.17. 開発者向けのオプション\"]\n",
      "3: ref=parallel-plans.html, titles=[\"PostgreSQL 15.4文書\", \"15.3. パラレルプラン\", \"15.3.4. パラレルアペンド\"]\n",
      "4: ref=runtime-config-resource.html, titles=[\"PostgreSQL 15.4文書\", \"20.4. 資源の消費\", \"20.4.6. 非同期動作\"]\n",
      "5: ref=parallel-plans.html, titles=[\"PostgreSQL 15.4文書\", \"15.3. パラレルプラン\", \"15.3.5. パラレルプランに関するヒント\"]\n",
      "\n",
      "========== LLM output ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- パラレルプランの改善: パラレルプランの生成と実行の効率が向上しました。また、パラレルプランに関するヒントの使用も可能になりました。\n",
      "- パラレルアペンドの無効化: パラレルアペンドを無効にするための設定値 `enable_parallel_append` が追加されました。\n",
      "- 非同期動作の制御: パラレルクエリの実行に関連するリソース消費量を制御するための設定値 `max_parallel_workers_per_gather` が追加されました。\n",
      "- 開発者向けのオプションの追加: 開発者がテストやデバッグの目的で使用できるオプションが追加されました。例えば、`force_parallel_mode`、`ignore_system_indexes`、`post_auth_delay`、`pre_auth_delay`、`trace_notify`などがあります。\n",
      "- システムインデックスの無視: システムテーブルの読み込み時にシステムインデックスを無視するための設定値 `ignore_system_indexes` が追加されました。\n",
      "- デバッグ出力の制御: `trace_notify` を使用して、LISTENとNOTIFYコマンドのデバッグ出力を生成することができます。\n",
      "\n",
      "以上が、PostgreSQL 15で実装された新しい機能の一部です。詳細な情報は、関連する出典を参照してください。\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Maximum number of context documents embedded in a prompt\n",
    "MAX_CONTEXT_DOC_NUM = 64\n",
    "\n",
    "prompt_to_generate_search_words_in_kb = f\"\"\"\n",
    "ユーザからの質問:\n",
    "{qes}\n",
    "\n",
    "上記のPostgreSQLに関する質問に回答するためにはナレッジベースを検索して回答する必要があります。\n",
    "新たな質問を元に、ナレッジベースを検索するための質問文を生成してください。\n",
    "質問文は日本語で生成してください。\n",
    "出力内容は質問文のみを含めるようにしてください。\n",
    "質問文に[]または<<>>で囲まれた文字列を含めないでください。\n",
    "質問文に'+'のような特殊文字を含めないでください。\n",
    "質問文をダブルクォーテーションで囲わないでください。\n",
    "質問文を生成できない場合は、数字の0だけを返してください。\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Improve search quality with prompt engineering\n",
    "search_words_in_kb = llm.invoke(system_template).content\n",
    "docs = index.similarity_search(search_words_in_kb, k=MAX_CONTEXT_DOC_NUM)\n",
    "# docs = index.similarity_search(qes, k=MAX_CONTEXT_DOC_NUM)\n",
    "\n",
    "# Tweak context length against the maximum number of LLM tokens\n",
    "num_context_tokens = count_tokens(template)\n",
    "context_docs = []\n",
    "for doc in docs:\n",
    "    num_tokens = count_tokens(doc.page_content)\n",
    "    if num_context_tokens + num_tokens > MAX_NUM_TOKENS:\n",
    "        break\n",
    "        \n",
    "    num_context_tokens += num_tokens\n",
    "    context_docs.append(doc)\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in context_docs])\n",
    "\n",
    "system_template = f\"\"\"\n",
    "アシスタントはユーザからの質問をサポートします。\n",
    "詳細な回答を心がけてください。\n",
    "日本語で回答してください。\n",
    "以下にリストされた出典に記載されている事実のみを用いて回答してください。\n",
    "以下の情報が十分でない場合は、分からないと回答してください。\n",
    "以下に示す関連する出典を用いない回答は作成しないでください。\n",
    "\n",
    "関連する出典:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    HumanMessagePromptTemplate.from_template('{input}')\n",
    "])\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt, memory=ConversationBufferMemory(memory_key='history', return_messages=True))\n",
    "\n",
    "print(f\"\\n========== Context (n={len(context_docs)}, num_tokens={num_context_tokens}) ==========\\n\")\n",
    "for i, doc in enumerate(context_docs):\n",
    "    print(\"{}: ref={}, titles={}\".format(i, doc.metadata['ref'], doc.metadata['titles']))\n",
    "\n",
    "print(\"\\n========== LLM output ==========\\n\")\n",
    "print(llm_chain.run(input=qes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "97ff32ed-43b1-481e-b753-e0bc9a53309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='PostgreSQL 15で実装された新しい機能を箇条書きで可能な限り教えてください'), AIMessage(content='- パラレルプランの改善: パラレルプランの生成と実行の効率が向上しました。また、パラレルプランに関するヒントの使用も可能になりました。\\n- パラレルアペンドの無効化: パラレルアペンドを無効にするための設定値 `enable_parallel_append` が追加されました。\\n- 非同期動作の制御: パラレルクエリの実行に関連するリソース消費量を制御するための設定値 `max_parallel_workers_per_gather` が追加されました。\\n- 開発者向けのオプションの追加: 開発者がテストやデバッグの目的で使用できるオプションが追加されました。例えば、`force_parallel_mode`、`ignore_system_indexes`、`post_auth_delay`、`pre_auth_delay`、`trace_notify`などがあります。\\n- システムインデックスの無視: システムテーブルの読み込み時にシステムインデックスを無視するための設定値 `ignore_system_indexes` が追加されました。\\n- デバッグ出力の制御: `trace_notify` を使用して、LISTENとNOTIFYコマンドのデバッグ出力を生成することができます。\\n\\n以上が、PostgreSQL 15で実装された新しい機能の一部です。詳細な情報は、関連する出典を参照してください。')]), return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(llm_chain.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c34811-54d9-4da1-b1fd-20498007cdc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
